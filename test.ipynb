{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc488aa",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "This notebook is designed to work in Google Colab for data mining tasks.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FarnoodTavasoli/datamining_project/blob/main/data_mining_project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21830834",
   "metadata": {},
   "source": [
    "## Setup for Google Colab\n",
    "\n",
    "This section sets up the environment when running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d1bd6",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "Loading the Ionosphere dataset and performing initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ionosphere dataset\n",
    "if IN_COLAB:\n",
    "    # Update this path to point to your uploaded files folder in Google Drive\n",
    "    data_path = '/content/drive/MyDrive/datamining_project/ionosphere.data'\n",
    "else:\n",
    "    # Local path\n",
    "    data_path = 'files/ionosphere_5/ionosphere.data'\n",
    "\n",
    "# Column names for the dataset\n",
    "# 34 continuous features + 1 target variable\n",
    "column_names = [f'feature_{i}' for i in range(1, 35)] + ['class']\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path, header=None, names=column_names)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nNumber of instances: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sum())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary of Features:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "df['class'].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df['class'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "axes[1].set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f959a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions (all 34 features)\n",
    "num_features = 34\n",
    "n_cols = 6\n",
    "n_rows = int(np.ceil(num_features / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 3))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(num_features):\n",
    "    feature_name = f'feature_{i+1}'\n",
    "    axes[i].hist(df[feature_name], bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    axes[i].set_title(f'{feature_name}', fontsize=9)\n",
    "    axes[i].set_xlabel('Value', fontsize=8)\n",
    "    axes[i].set_ylabel('Frequency', fontsize=8)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(num_features, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle('Distribution of All 34 Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (all 34 features)\n",
    "plt.figure(figsize=(18, 14))\n",
    "correlation_matrix = df.iloc[:, :34].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,\n",
    "            linewidths=0.3, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation Heatmap (All 34 Features)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f3b4b",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Preparing the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Encode target variable (g=good, b=bad)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y_encoded.shape}\")\n",
    "print(f\"\\nClass encoding:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"\\nScaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test data shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nSample scaled features (first 5):\")\n",
    "print(X_train_scaled[:5, :5])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
