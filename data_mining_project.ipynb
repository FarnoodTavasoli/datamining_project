{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FarnoodTavasoli/datamining_project/blob/main/data_mining_project.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score,\n",
    "                             f1_score, confusion_matrix)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "Loading the Ionosphere dataset and performing initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ionosphere dataset\n",
    "if IN_COLAB:\n",
    "    data_path = '/content/drive/MyDrive/datamining_project/ionosphere.data'\n",
    "else:\n",
    "    data_path = 'files/ionosphere_5/ionosphere.data'\n",
    "\n",
    "\n",
    "column_names = [f'feature_{i}' for i in range(1, 35)] + ['class']\n",
    "\n",
    "df = pd.read_csv(data_path, header=None, names=column_names)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset info\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Rows: {df.shape[0]}\")\n",
    "print(f\"Features: {df.shape[1] - 1}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum().sum())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "df['class'].value_counts().plot(\n",
    "    kind='bar',\n",
    "    ax=axes[0],\n",
    "    color=['#2ecc71', '#e74c3c']\n",
    ")\n",
    "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df['class'].value_counts().plot(\n",
    "    kind='pie',\n",
    "    ax=axes[1],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['#2ecc71', '#e74c3c'],\n",
    "    startangle=90\n",
    ")\n",
    "axes[1].set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "num_features = df.shape[1] - 1\n",
    "n_cols = 6\n",
    "n_rows = int(np.ceil(num_features / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 3))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(num_features):\n",
    "    feature_name = f'feature_{i+1}'\n",
    "    axes[i].hist(\n",
    "        df[feature_name],\n",
    "        bins=30,\n",
    "        alpha=0.7,\n",
    "        color='steelblue',\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[i].set_title(f'{feature_name}', fontsize=9)\n",
    "    axes[i].set_xlabel('Value', fontsize=8)\n",
    "    axes[i].set_ylabel('Frequency', fontsize=8)\n",
    "\n",
    "# remove empty subplots\n",
    "for j in range(num_features, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle('Distribution of Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(18, 14))\n",
    "correlation_matrix = df.iloc[:, :34].corr()\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=False,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={'label': 'Correlation'}\n",
    ")\n",
    "plt.title('Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Preparing the data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Encode target variable (g=good, b=bad)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"\\nClass encoding:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features (no variety)\n",
    "\n",
    "constant_features = []\n",
    "for column in X.columns:\n",
    "    if X[column].nunique() == 1:\n",
    "        constant_features.append(column)\n",
    "\n",
    "if constant_features:\n",
    "    print(f\"Found {len(constant_features)} constant feature(s) with no variety:\")\n",
    "    for feature in constant_features:\n",
    "        print(f\"  - {feature}: {X[feature].unique()[0]}\")\n",
    "    \n",
    "    X = X.drop(columns=constant_features)\n",
    "    print(f\"\\nConstant features dropped!\")\n",
    "    print(f\"Features shape after removing constant features: {X.shape}\")\n",
    "else:\n",
    "    print(\"No constant features found. All features have variety.\")\n",
    "\n",
    "print(f\"\\nRemaining features shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for train and test (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"\\nScaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Results Visualization\n",
    "\n",
    "Visualizing how the data changed after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs scaled features\n",
    "comparison_stats = pd.DataFrame({\n",
    "    'Feature': [f'F{i+1}' for i in range(X_train.shape[1])],\n",
    "    'Original Mean': X_train.mean().values,\n",
    "    'Original Std': X_train.std().values,\n",
    "    'Scaled Mean': X_train_scaled.mean(axis=0),\n",
    "    'Scaled Std': X_train_scaled.std(axis=0)\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA STATISTICS: ORIGINAL vs SCALED (All Features)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_stats.round(4).to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Statistics of scaled data\n",
    "print(\"=\"*80)\n",
    "print(\"SCALED TRAINING DATA SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Overall Mean (should be ~0): {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Overall Std Dev (should be ~1): {X_train_scaled.std():.6f}\")\n",
    "print(f\"Min value: {X_train_scaled.min():.4f}\")\n",
    "print(f\"Max value: {X_train_scaled.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary table\n",
    "print(\"=\"*80)\n",
    "print(\"DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Total Samples',\n",
    "        'Training Samples',\n",
    "        'Test Samples',\n",
    "        'Number of Features',\n",
    "        'Class 0 (Bad)',\n",
    "        'Class 1 (Good)',\n",
    "        'Feature Scaling',\n",
    "        'Scaled Data Mean',\n",
    "        'Scaled Data Std Dev'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{len(df)}\",\n",
    "        f\"{len(X_train)}\",\n",
    "        f\"{len(X_test)}\",\n",
    "        f\"{X_train.shape[1]}\",\n",
    "        f\"{(y_train == 0).sum()}\",\n",
    "        f\"{(y_train == 1).sum()}\",\n",
    "        \"StandardScaler\",\n",
    "        f\"{X_train_scaled.mean():.6f}\",\n",
    "        f\"{X_train_scaled.std():.6f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Split proportions (bar plot)\n",
    "split_labels = ['Train Set', 'Test Set']\n",
    "split_sizes = [len(X_train), len(X_test)]\n",
    "colors_split = ['#3498db', '#e74c3c']\n",
    "\n",
    "axes[0].bar(\n",
    "    split_labels,\n",
    "    split_sizes,\n",
    "    color=colors_split,\n",
    "    edgecolor='black',\n",
    "    linewidth=2\n",
    ")\n",
    "axes[0].set_title('Train/Test Split Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Samples', fontsize=11)\n",
    "for i, v in enumerate(split_sizes):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Class distribution in train and test sets\n",
    "train_class_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "test_class_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "class_names = ['Bad (0)', 'Good (1)']\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(\n",
    "    x - width/2,\n",
    "    train_class_dist.values,\n",
    "    width,\n",
    "    label='Train Set',\n",
    "    color='#3498db',\n",
    "    edgecolor='black'\n",
    ")\n",
    "axes[1].bar(\n",
    "    x + width/2,\n",
    "    test_class_dist.values,\n",
    "    width,\n",
    "    label='Test Set',\n",
    "    color='#e74c3c',\n",
    "    edgecolor='black'\n",
    ")\n",
    "axes[1].set_title('Class Distribution in Train/Test Sets', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Samples', fontsize=11)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(class_names)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize boxplots comparing original vs scaled data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original data boxplot\n",
    "num_features = X_train.shape[1]\n",
    "axes[0].boxplot(\n",
    "    [X_train.iloc[:, i] for i in range(num_features)],\n",
    "    labels=[f'F{i+1}' for i in range(num_features)]\n",
    ")\n",
    "axes[0].set_title(f'Original Features Boxplot (All {num_features} Features)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_xlabel('Features')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Scaled data boxplot\n",
    "axes[1].boxplot(\n",
    "    [X_train_scaled[:, i] for i in range(num_features)],\n",
    "    labels=[f'F{i+1}' for i in range(num_features)]\n",
    ")\n",
    "axes[1].set_title(f'Scaled Features Boxplot (All {num_features} Features)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_xlabel('Features')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ALL feature distributions\n",
    "num_features = X_train.shape[1]\n",
    "num_cols = 4\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 4*num_rows))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(num_features):\n",
    "    # Plot original\n",
    "    axes[i].hist(\n",
    "        X_train.iloc[:, i],\n",
    "        bins=30,\n",
    "        alpha=0.6,\n",
    "        label='Original',\n",
    "        color='steelblue',\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    # Plot scaled\n",
    "    ax2 = axes[i].twinx()\n",
    "    ax2.hist(\n",
    "        X_train_scaled[:, i],\n",
    "        bins=30,\n",
    "        alpha=0.6,\n",
    "        label='Scaled',\n",
    "        color='orange',\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f'Feature {i+1}', fontsize=10, fontweight='bold')\n",
    "    axes[i].set_xlabel('Value', fontsize=9)\n",
    "    axes[i].set_ylabel('Frequency (Original)', color='steelblue', fontsize=9)\n",
    "    ax2.set_ylabel('Frequency (Scaled)', color='orange', fontsize=9)\n",
    "    axes[i].tick_params(axis='y', labelcolor='steelblue')\n",
    "    ax2.tick_params(axis='y', labelcolor='orange')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# delete empty subplots\n",
    "for i in range(num_features, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'All {num_features} Features: Original vs Scaled Distributions', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation effect on Decision Tree\n",
    "\n",
    "Compare four Decision Tree training strategies and evaluate them on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPR = Recall, FPR = FP / (FP + TN)\n",
    "\n",
    "def evaluate_on_test(model, X_test, y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "    return {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'TPR (Recall)': tpr,\n",
    "        'FPR': fpr,\n",
    "        'F1': f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DECISION TREE: 4 TRAINING STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1) Normal training\n",
    "base_tree = DecisionTreeClassifier()\n",
    "base_tree.fit(X_train, y_train)\n",
    "\n",
    "# 2) Training with 10-fold CV\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "cv_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "cv_search.fit(X_train, y_train)\n",
    "cv_tree = cv_search.best_estimator_\n",
    "cv_scores = cv_search.cv_results_['mean_test_score']\n",
    "\n",
    "# 3) Normal training with CCP post-pruning\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train\n",
    ")\n",
    "path = DecisionTreeClassifier().cost_complexity_pruning_path(X_train_sub, y_train_sub)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "val_scores = []\n",
    "for alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "    clf.fit(X_train_sub, y_train_sub)\n",
    "    val_scores.append(accuracy_score(y_val, clf.predict(X_val)))\n",
    "\n",
    "best_alpha_val = ccp_alphas[int(np.argmax(val_scores))]\n",
    "ccp_tree = DecisionTreeClassifier(ccp_alpha=best_alpha_val)\n",
    "ccp_tree.fit(X_train, y_train)\n",
    "\n",
    "# 4) 10-fold CV with CCP\n",
    "cv_alpha_scores = []\n",
    "for alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_alpha_scores.append(scores.mean())\n",
    "\n",
    "best_alpha_cv = ccp_alphas[int(np.argmax(cv_alpha_scores))]\n",
    "ccp_cv_tree = DecisionTreeClassifier(ccp_alpha=best_alpha_cv)\n",
    "ccp_cv_tree.fit(X_train, y_train)\n",
    "\n",
    "# Compare all models\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Normal Training',\n",
    "    'CV Accuracy (train)': None,\n",
    "    'CCP Alpha': None,\n",
    "    **evaluate_on_test(base_tree, X_test, y_test)\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Model': '10-Fold CV',\n",
    "    'CV Accuracy (train)': cv_search.best_score_,\n",
    "    'CCP Alpha': None,\n",
    "    **evaluate_on_test(cv_tree, X_test, y_test)\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Model': 'Normal + CCP',\n",
    "    'CV Accuracy (train)': None,\n",
    "    'CCP Alpha': best_alpha_val,\n",
    "    **evaluate_on_test(ccp_tree, X_test, y_test)\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    'Model': '10-Fold CV + CCP',\n",
    "    'CV Accuracy (train)': max(cv_alpha_scores),\n",
    "    'CCP Alpha': best_alpha_cv,\n",
    "    **evaluate_on_test(ccp_cv_tree, X_test, y_test)\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df['CV Accuracy (train)'] = results_df['CV Accuracy (train)'].round(6)\n",
    "results_df['CCP Alpha'] = results_df['CCP Alpha'].apply(lambda x: None if x is None else round(float(x), 8))\n",
    "for col in ['Accuracy', 'Precision', 'TPR (Recall)', 'FPR','F1']:\n",
    "    results_df[col] = results_df[col].round(6)\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Highlight best model by test accuracy\n",
    "best_model = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(\"\\nBest model by test accuracy:\")\n",
    "print(best_model.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Visualization\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'TPR (Recall)', 'FPR', 'F1']\n",
    "models = results_df['Model'].tolist()\n",
    "\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.18\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i, model in enumerate(models):\n",
    "    scores = results_df.loc[results_df['Model'] == model, metrics_to_plot].values.flatten()\n",
    "    ax.bar(x + i * width, scores, width, label=model)\n",
    "\n",
    "ax.set_title('Model Comparison on Test Set Metrics', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xticks(x + (width * (len(models) - 1) / 2))\n",
    "ax.set_xticklabels(metrics_to_plot, rotation=0)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(ncol=2)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods: AdaBoost & Random Forest\n",
    "\n",
    "Comparing ensemble strategies using AdaBoost and Random Forest with and without CCP pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENSEMBLE METHODS: 4 TRAINING STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get CCP alphas from a single tree\n",
    "X_ens_train, X_ens_val, y_ens_train, y_ens_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train\n",
    ")\n",
    "path = DecisionTreeClassifier().cost_complexity_pruning_path(X_ens_train, y_ens_train)\n",
    "ccp_alphas_ens = path.ccp_alphas\n",
    "\n",
    "\n",
    "# Finding alpha for Ada\n",
    "ada_val_scores = []\n",
    "for alpha in ccp_alphas_ens:\n",
    "    base_est = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "    ada_clf = AdaBoostClassifier(estimator=base_est, n_estimators=50, algorithm='SAMME')\n",
    "    ada_clf.fit(X_ens_train, y_ens_train)\n",
    "    ada_val_scores.append(accuracy_score(y_ens_val, ada_clf.predict(X_ens_val)))\n",
    "\n",
    "best_alpha_ada = ccp_alphas_ens[int(np.argmax(ada_val_scores))]\n",
    "\n",
    "# Finding alpha for Random Forest\n",
    "rf_val_scores = []\n",
    "for alpha in ccp_alphas_ens:\n",
    "    base_est = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=None, min_impurity_decrease=alpha)\n",
    "    rf_clf.fit(X_ens_train, y_ens_train)\n",
    "    rf_val_scores.append(accuracy_score(y_ens_val, rf_clf.predict(X_ens_val)))\n",
    "\n",
    "best_alpha_rf = ccp_alphas_ens[int(np.argmax(rf_val_scores))]\n",
    "\n",
    "# 1) AdaBoost Normal\n",
    "ada_base = AdaBoostClassifier(n_estimators=50, algorithm='SAMME')\n",
    "ada_base.fit(X_train, y_train)\n",
    "\n",
    "# 2) AdaBoost Pruned\n",
    "\n",
    "ada_pruned = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(ccp_alpha=best_alpha_ada),\n",
    "    n_estimators=50,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "ada_pruned.fit(X_train, y_train)\n",
    "\n",
    "# 3) Random Forest Normal\n",
    "rf_base = RandomForestClassifier(n_estimators=100)\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "# 4) Random Forest Pruned\n",
    "rf_pruned = RandomForestClassifier(n_estimators=100, min_impurity_decrease=best_alpha_rf)\n",
    "rf_pruned.fit(X_train, y_train)\n",
    "\n",
    "ens_results = []\n",
    "\n",
    "ens_results.append({\n",
    "    'Model': 'AdaBoost (default)',\n",
    "    'CCP Alpha': None,\n",
    "    **evaluate_on_test(ada_base, X_test, y_test)\n",
    "})\n",
    "\n",
    "ens_results.append({\n",
    "    'Model': 'AdaBoost (Pruned)',\n",
    "    'CCP Alpha': best_alpha_ada,\n",
    "    **evaluate_on_test(ada_pruned, X_test, y_test)\n",
    "})\n",
    "\n",
    "ens_results.append({\n",
    "    'Model': 'Random Forest (default)',\n",
    "    'CCP Alpha': None,\n",
    "    **evaluate_on_test(rf_base, X_test, y_test)\n",
    "})\n",
    "\n",
    "ens_results.append({\n",
    "    'Model': 'Random Forest (Pruned)',\n",
    "    'CCP Alpha': best_alpha_rf,\n",
    "    **evaluate_on_test(rf_pruned, X_test, y_test)\n",
    "})\n",
    "\n",
    "ens_results_df = pd.DataFrame(ens_results)\n",
    "\n",
    "# Results\n",
    "ens_results_df['CCP Alpha'] = ens_results_df['CCP Alpha'].apply(lambda x: None if x is None else round(float(x), 8))\n",
    "for col in ['Accuracy', 'Precision', 'TPR (Recall)', 'FPR',  'F1']:\n",
    "    ens_results_df[col] = ens_results_df[col].round(6)\n",
    "\n",
    "print(ens_results_df.to_string(index=False))\n",
    "\n",
    "# Highlight best model\n",
    "best_ens = ens_results_df.loc[ens_results_df['Accuracy'].idxmax()]\n",
    "print(\"Best ensemble model by test accuracy:\")\n",
    "print(best_ens.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensemble model comparison visualization\n",
    "ens_metrics = ['Accuracy', 'Precision', 'TPR (Recall)', 'FPR', 'F1']\n",
    "ens_models = ens_results_df['Model'].tolist()\n",
    "\n",
    "x = np.arange(len(ens_metrics))\n",
    "width = 0.18\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i, model in enumerate(ens_models):\n",
    "    scores = ens_results_df.loc[ens_results_df['Model'] == model, ens_metrics].values.flatten()\n",
    "    ax.bar(x + i * width, scores, width, label=model)\n",
    "\n",
    "ax.set_title('Ensemble Methods: Model Comparison on Test Set', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xticks(x + (width * (len(ens_models) - 1) / 2))\n",
    "ax.set_xticklabels(ens_metrics, rotation=0)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(ncol=2)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering (K-Means vs K-Medoids)\n",
    "comparing cluster assignments using misclassification percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_scaled = StandardScaler().fit_transform(df.drop('class', axis=1).drop(columns=df.columns[df.nunique() == 1]))\n",
    "y_true = LabelEncoder().fit_transform(df['class'])  # b=0, g=1\n",
    "\n",
    "k_values = [2, 3, 4, 5]\n",
    "\n",
    "def compute_misclassification(y_true, labels, k):\n",
    "    \"\"\"misclassification percent by comparing cluster assignments to actual labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        labels: Cluster labels.\n",
    "        k: Number of clusters.\n",
    "\n",
    "    Returns:\n",
    "        Misclassification percentage.\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_true = np.unique(y_true)\n",
    "    unique_clusters = np.unique(labels)\n",
    "\n",
    "    best_error = len(y_true)\n",
    "\n",
    "    if k == len(unique_true):\n",
    "        # Try all permutations of label assignment\n",
    "        for perm in permutations(unique_true):\n",
    "            mapping = dict(zip(unique_clusters, perm))\n",
    "            mapped = np.array([mapping.get(c, -1) for c in labels])\n",
    "            errors = np.sum(mapped != y_true)\n",
    "            if errors < best_error:\n",
    "                best_error = errors\n",
    "    else:\n",
    "        mapped = np.zeros_like(labels)\n",
    "        for c in unique_clusters:\n",
    "            mask = labels == c\n",
    "            if np.sum(mask) > 0:\n",
    "                majority = np.bincount(y_true[mask]).argmax()\n",
    "                mapped[mask] = majority\n",
    "        best_error = np.sum(mapped != y_true)\n",
    "\n",
    "    return (best_error / len(y_true)) * 100\n",
    "\n",
    "\n",
    "def k_medoids(X, k, max_iter=100, random_state=42):\n",
    "    \"\"\" K-Medoids clustering algorithm\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        k: Number of clusters.\n",
    "        max_iter: Maximum iterations.\n",
    "        random_state: Random seed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (labels, medoid_indices)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # distance matrix\n",
    "    dist = np.linalg.norm(X[:, None, :] - X[None, :, :], axis=2)\n",
    "\n",
    "    medoid_indices = rng.choice(n, size=k, replace=False)\n",
    "    labels = None\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Assign points to nearest medoid\n",
    "        distances_to_medoids = dist[:, medoid_indices]\n",
    "        new_labels = np.argmin(distances_to_medoids, axis=1)\n",
    "\n",
    "        # Update medoids\n",
    "        new_medoids = medoid_indices.copy()\n",
    "        for j in range(k):\n",
    "            cluster_idx = np.where(new_labels == j)[0]\n",
    "            if len(cluster_idx) == 0:\n",
    "                new_medoids[j] = rng.choice(n)\n",
    "                continue\n",
    "            cluster_dist = dist[np.ix_(cluster_idx, cluster_idx)]\n",
    "            medoid_local = cluster_idx[np.argmin(cluster_dist.sum(axis=1))]\n",
    "            new_medoids[j] = medoid_local\n",
    "\n",
    "        if labels is not None and np.array_equal(new_labels, labels) and np.array_equal(new_medoids, medoid_indices):\n",
    "            labels = new_labels\n",
    "            medoid_indices = new_medoids\n",
    "            break\n",
    "\n",
    "        labels = new_labels\n",
    "        medoid_indices = new_medoids\n",
    "\n",
    "    return labels, medoid_indices\n",
    "\n",
    "\n",
    "# Run clustering for each k\n",
    "results = []\n",
    "kmeans_models = {}\n",
    "kmedoids_models = {}\n",
    "\n",
    "for k in k_values:\n",
    "    # K-Means\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    km_labels = km.fit_predict(X_all_scaled)\n",
    "    km_error = compute_misclassification(y_true, km_labels, k)\n",
    "    kmeans_models[k] = km_labels\n",
    "\n",
    "    # K-Medoids\n",
    "    kmed_labels, _ = k_medoids(X_all_scaled, k, max_iter=100, random_state=42)\n",
    "    kmed_error = compute_misclassification(y_true, kmed_labels, k)\n",
    "    kmedoids_models[k] = kmed_labels\n",
    "\n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'K-Means Error (%)': round(km_error, 2),\n",
    "        'K-Medoids Error (%)': round(kmed_error, 2)\n",
    "    })\n",
    "\n",
    "cluster_df = pd.DataFrame(results)\n",
    "print(\"Clustering Misclassification Rates:\")\n",
    "print(cluster_df.to_string(index=False),\"\\n\")\n",
    "\n",
    "display(\n",
    "    cluster_df.style.set_caption(\"Misclassification % by Clustering Method and k\")\n",
    "    .set_properties(**{'text-align': 'center'})\n",
    "    .highlight_min(\n",
    "        subset=['K-Means Error (%)', 'K-Medoids Error (%)'],\n",
    "        color='lightgreen'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster visualization\n",
    "# Reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_all_scaled)\n",
    "\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "for k in k_values:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(f'Clustering Comparison â€” k = {k}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # K-Means\n",
    "    ax = axes[0]\n",
    "    km_labels = kmeans_models[k]\n",
    "    for c in range(k):\n",
    "        mask = km_labels == c\n",
    "        ax.scatter(\n",
    "            X_pca[mask, 0],\n",
    "            X_pca[mask, 1],\n",
    "            c=colors[c],\n",
    "            label=f'Cluster {c}',\n",
    "            alpha=0.6,\n",
    "            edgecolors='w',\n",
    "            linewidth=0.3,\n",
    "            s=30\n",
    "        )\n",
    "    ax.set_title(f'K-Means (k={k})\\nError: {cluster_df.loc[cluster_df[\"k\"]==k, \"K-Means Error (%)\"].values[0]}%')\n",
    "    ax.set_xlabel('PCA Component 1')\n",
    "    ax.set_ylabel('PCA Component 2')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # K-Medoids\n",
    "    ax = axes[1]\n",
    "    kmed_labels = kmedoids_models[k]\n",
    "    for c in range(k):\n",
    "        mask = kmed_labels == c\n",
    "        ax.scatter(\n",
    "            X_pca[mask, 0],\n",
    "            X_pca[mask, 1],\n",
    "            c=colors[c],\n",
    "            label=f'Cluster {c}',\n",
    "            alpha=0.6,\n",
    "            edgecolors='w',\n",
    "            linewidth=0.3,\n",
    "            s=30\n",
    "        )\n",
    "    ax.set_title(f'K-Medoids (k={k})\\nError: {cluster_df.loc[cluster_df[\"k\"]==k, \"K-Medoids Error (%)\"].values[0]}%')\n",
    "    ax.set_xlabel('PCA Component 1')\n",
    "    ax.set_ylabel('PCA Component 2')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
